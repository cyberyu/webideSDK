{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cf7343e-02ce-411a-b516-4b2a9c2693d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/syu2/anaconda3/envs/refact3/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Loading checkpoint shards:  50%|████████████████████████████████████████████████████████████▌                                                            | 1/2 [00:04<00:04,  4.69s/it]"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from peft import PeftModel\n",
    "\n",
    "\n",
    "#Base model on your local filesystem\n",
    "base_model_dir = \"/home/syu2/.refact/perm-storage/weights/models--codellama--CodeLlama-7b-hf/snapshots/6c284d1468fe6c413cf56183e69b194dcfa27fe6/\"\n",
    "base_model = AutoModelForCausalLM.from_pretrained(base_model_dir)\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model_dir)\n",
    "\n",
    "#Adaptor directory on your local filesystem\n",
    "adaptor_dir = \"/home/syu2/.refact/perm-storage/loras/tbkcode_experiment_01/checkpoints/iter1050-testloss0.676/\"\n",
    "merged_model = PeftModel.from_pretrained(base_model,adaptor_dir)\n",
    "\n",
    "merged_model = merged_model.merge_and_unload()\n",
    "merged_model.save_pretrained(\"/home/syu2/.refact/perm-storage/loras/tbkcode_experiment_01/merged_model_iteration1005\")\n",
    "tokenizer.save_pretrained(\"/home/syu2/.refact/perm-storage/loras/tbkcode_experiment_01/merged_model_iteration1005\", legacy_format=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "013a863c-3703-4a87-afa5-055b7223c493",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b423a4d791bd4b37916d5a72613330fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s> Show me an example of sorting algorithm in python?\n",
      "\n",
      "I am trying to implement sorting algorithm in python. I have tried to implement bubble sort, but it is not working.\n",
      "\n",
      "\\begin{code}\n",
      "def bubble_sort(arr):\n",
      "    n = len(arr)\n",
      "    for i in range(n-1):\n",
      "        for j in range(n-i-1):\n",
      "            if arr[j] > arr[j+1]:\n",
      "                arr[j], arr[j+1] = arr[j+1], arr[j]\n",
      "    return arr\n",
      "\n",
      "arr = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
      "print(bubble_sort(arr))\n",
      "\\end{code}\n",
      "\n",
      "Comment: What do you mean by \"not working\"?\n",
      "\n",
      "Comment: It is not sorting the array.\n",
      "\n",
      "Comment: What is the expected output?\n",
      "\n",
      "Comment: The output should be sorted array.\n",
      "\n",
      "Comment: What is the output you are getting?\n",
      "\n",
      "Comment: The output is same as input.\n",
      "\n",
      "Comment: I am getting the same output as input.\n",
      "\n",
      "Comment: I am getting the same output as input.\n",
      "\n",
      "Comment: I am getting the same output as input.\n",
      "\n",
      "Comment: I am getting the same output as input.\n",
      "\n",
      "Comment: I am getting the same output as input.\n",
      "\n",
      "Comment: I am getting the same output as input.\n",
      "\n",
      "Comment: I am getting the same output as input.\n",
      "\n",
      "Comment: I am getting the same output as input.\n",
      "\n",
      "Comment: I am getting the same output as input.\n",
      "\n",
      "Comment: I am getting the same output as input.\n",
      "\n",
      "Comment: I am getting the same output as input.\n",
      "\n",
      "Comment: I am getting the same output as input.\n",
      "\n",
      "Comment: I am getting the same output as input.\n",
      "\n",
      "Comment: I am getting the same output as input.\n",
      "\n",
      "Comment: I am getting the same output as input.\n",
      "\n",
      "Comment: I am getting the same output as input.\n",
      "\n",
      "Comment: I am getting the same output as input.\n",
      "\n",
      "Comment: I am getting the same output as input.\n",
      "\n",
      "Comment: I am getting the same output as input.\n",
      "\n",
      "Comment: I am getting the same output as input.\n",
      "\n",
      "Comment: I am getting the same output as input.\n",
      "\n",
      "Comment\n"
     ]
    }
   ],
   "source": [
    "# Test the merged model\n",
    "\n",
    "import pandas as pd\n",
    "import time\n",
    "import requests\n",
    "import json\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, TextStreamer, BitsAndBytesConfig\n",
    "import torch\n",
    "import re\n",
    "import os\n",
    "import pickle\n",
    "import re\n",
    "\n",
    "\n",
    "\n",
    "MERGED_MODEL_NAME = \"/home/syu2/.refact/perm-storage/loras/tbkcode_experiment_01/merged_model_iteration1005/\"\n",
    "#BASE_MODEL = \"meta-llama/Llama-2-7b-hf\"\n",
    "\n",
    "#config = PeftConfig.from_pretrained(MODEL_NAME)\n",
    "\n",
    "#tokenizer = LlamaTokenizer.from_pretrained(BASE_MODEL)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MERGED_MODEL_NAME)\n",
    "model = AutoModelForCausalLM.from_pretrained(MERGED_MODEL_NAME, load_in_8bit=True, device_map='cuda:0')\n",
    "\n",
    "\n",
    "final_prompt = 'Show me an example of sorting algorithm in python'\n",
    "\n",
    "try:\n",
    "    input_ids = tokenizer(final_prompt, return_tensors=\"pt\").to(\"cuda:0\")\n",
    "    outputs = model.generate(**input_ids, max_new_tokens=512, pad_token_id=128001)\n",
    "    response = tokenizer.decode(outputs[0])\n",
    "    print(response)\n",
    "except Exception as e:\n",
    "    print(f\"[make_request_llama3Instruct] Exception: {e}\")\n",
    "   \n",
    "#model = PeftModel.from_pretrained(model, MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d0630241-4840-4989-8e77-fdd4ff127ffa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "95db8b2e42c34197863035d264b9c36b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s> Show me an example of sorting algorithm in python?\n",
      "\n",
      "I am trying to write a sorting algorithm in python. I have written the following code:\n",
      "\n",
      "\\begin{code}\n",
      "def bubble_sort(array):\n",
      "    for i in range(len(array)):\n",
      "        for j in range(len(array)-1):\n",
      "            if array[j] > array[j+1]:\n",
      "                array[j], array[j+1] = array[j+1], array[j]\n",
      "    return array\n",
      "\\end{code}\n",
      "\n",
      "I am getting the following error:\n",
      "\n",
      "\\begin{code}\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Saurabh\\Desktop\\Python\\bubble_sort.py\", line 11, in <module>\n",
      "    print(bubble_sort([1, 2, 3, 4, 5, 6, 7, 8, 9, 10]))\n",
      "  File \"C:\\Users\\Saurabh\\Desktop\\Python\\bubble_sort.py\", line 5, in bubble_sort\n",
      "    if array[j] > array[j+1]:\n",
      "IndexError: list index out of range\n",
      "\\end{code}\n",
      "\n",
      "Comment: You should use `range(len(array) - 1)` instead of `range(len(array)-1)`\n",
      "\n",
      "Comment: @SaurabhJain: You should accept the answer if it solved your problem.\n",
      "\n",
      "Answer: You should use `range(len(array) - 1)` instead of `range(len(array)-1)`\n",
      "\n",
      "Answer: You should use `range(len(array) - 1)` instead of `range(len(array)-1)`\n",
      "\n",
      "Answer: You should use `range(len(array) - 1)` instead of `range(len(array)-1)`\n",
      "\n",
      "Answer: You should use `range(len(array) - 1)` instead of `range(len(array)-1)`\n",
      "\n",
      "Answer: You should use `range(len(array) - 1)` instead of `range(len(array)-1)`\n",
      "\n",
      "Answer: You should use `range(len(array) - 1)` instead of `range(len(array)-1)`\n",
      "\n",
      "Comment: I am getting the following error:\n",
      "\n",
      "Traceback (most recent call\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "# save memory\n",
    "import torch\n",
    "torch.set_grad_enabled(False)\n",
    "\n",
    "model_path = \"/home/syu2/.refact/perm-storage/loras/tbkcode_experiment_01/merged_model_iteration1005_refact/\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path, use_safetensors=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_path, load_in_8bit=True, use_safetensors=True,  device_map='cuda:0')\n",
    "\n",
    "# # do you have enough vram to run it on gpu?  if so..\n",
    "# model.to(\"cuda:0\")\n",
    "\n",
    "input_string = \"Show me an example of sorting algorithm in python\"\n",
    "\n",
    "# tokenize to ids\n",
    "input_ids = tokenizer(input_string, return_tensors=\"pt\").to(\"cuda:0\")\n",
    "outputs = model.generate(**input_ids, max_new_tokens=512, pad_token_id=128001)\n",
    "response = tokenizer.decode(outputs[0])\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fa716998-3fb2-452b-a94b-a83dc2d20540",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9753de9e-f6b6-4739-b2e2-2f615ba52e51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are several ways to sort an array of numbers using Python. Here is one example:\n",
      "```python\n",
      "def sort_array(arr):\n",
      "    for i in range(len(arr) - 1)):\n",
      "        min_index = i\n",
      "        for j in range(i + 1, len(arr))):\n",
      "\n",
      "            if arr[min_index]] < arr[j]]:\n",
      "                min_index = j\n",
      "\n",
      "```\n",
      "\n",
      "This algorithm uses a \"bubble sort\" approach. The outer loop iterates through the array, while the inner loop compares adjacent elements and swaps them if necessary.\n",
      "This algorithm has a time complexity of O(n^2))\n"
     ]
    }
   ],
   "source": [
    "import ollama\n",
    "\n",
    "# generate a response combining the prompt and data we retrieved in step 2\n",
    "output = ollama.generate(\n",
    "  model=\"qwen:latest\",\n",
    "  prompt=f\"Write me a sorting algorithm in python\"\n",
    ")\n",
    "\n",
    "print(output['response'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a198d057-f956-47e1-9d50-70e0e9661419",
   "metadata": {},
   "outputs": [
    {
     "ename": "JSONDecodeError",
     "evalue": "Extra data: line 2 column 1 (char 7468)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mJSONDecodeError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mjson\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcomprehensive_evaluation_prompts.jsonl\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m data_file:    \n\u001b[0;32m----> 4\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[43mjson\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_file\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/refact3/lib/python3.9/json/__init__.py:293\u001b[0m, in \u001b[0;36mload\u001b[0;34m(fp, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[1;32m    274\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mload\u001b[39m(fp, \u001b[38;5;241m*\u001b[39m, \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, object_hook\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, parse_float\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    275\u001b[0m         parse_int\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, parse_constant\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, object_pairs_hook\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw):\n\u001b[1;32m    276\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Deserialize ``fp`` (a ``.read()``-supporting file-like object containing\u001b[39;00m\n\u001b[1;32m    277\u001b[0m \u001b[38;5;124;03m    a JSON document) to a Python object.\u001b[39;00m\n\u001b[1;32m    278\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    291\u001b[0m \u001b[38;5;124;03m    kwarg; otherwise ``JSONDecoder`` is used.\u001b[39;00m\n\u001b[1;32m    292\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 293\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mloads\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    294\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mobject_hook\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mobject_hook\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    295\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparse_float\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparse_float\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparse_int\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparse_int\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    296\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparse_constant\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparse_constant\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mobject_pairs_hook\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mobject_pairs_hook\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/refact3/lib/python3.9/json/__init__.py:346\u001b[0m, in \u001b[0;36mloads\u001b[0;34m(s, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[1;32m    341\u001b[0m     s \u001b[38;5;241m=\u001b[39m s\u001b[38;5;241m.\u001b[39mdecode(detect_encoding(s), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msurrogatepass\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    343\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m object_hook \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[1;32m    344\u001b[0m         parse_int \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m parse_float \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[1;32m    345\u001b[0m         parse_constant \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m object_pairs_hook \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m kw):\n\u001b[0;32m--> 346\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_default_decoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    347\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    348\u001b[0m     \u001b[38;5;28mcls\u001b[39m \u001b[38;5;241m=\u001b[39m JSONDecoder\n",
      "File \u001b[0;32m~/anaconda3/envs/refact3/lib/python3.9/json/decoder.py:340\u001b[0m, in \u001b[0;36mJSONDecoder.decode\u001b[0;34m(self, s, _w)\u001b[0m\n\u001b[1;32m    338\u001b[0m end \u001b[38;5;241m=\u001b[39m _w(s, end)\u001b[38;5;241m.\u001b[39mend()\n\u001b[1;32m    339\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m end \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(s):\n\u001b[0;32m--> 340\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m JSONDecodeError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExtra data\u001b[39m\u001b[38;5;124m\"\u001b[39m, s, end)\n\u001b[1;32m    341\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m obj\n",
      "\u001b[0;31mJSONDecodeError\u001b[0m: Extra data: line 2 column 1 (char 7468)"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "with open('comprehensive_evaluation_prompts.jsonl') as data_file:    \n",
    "    data = json.load(data_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d2448888-0176-4ffa-a83c-af0eb61d12ed",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "JSONDecodeError",
     "evalue": "Expecting value: line 1 column 2 (char 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mJSONDecodeError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfim_prompts_from_codebase.json\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m file:\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m file:\n\u001b[0;32m----> 6\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[43mjson\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloads\u001b[49m\u001b[43m(\u001b[49m\u001b[43mline\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstrip\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m         \u001b[38;5;66;03m#print(data['prompt'])\u001b[39;00m\n\u001b[1;32m      9\u001b[0m         output \u001b[38;5;241m=\u001b[39m ollama\u001b[38;5;241m.\u001b[39mgenerate(\n\u001b[1;32m     10\u001b[0m           model\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstarcoder2_20250704:latest\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     11\u001b[0m           prompt\u001b[38;5;241m=\u001b[39mdata[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprompt\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     12\u001b[0m         )   \n",
      "File \u001b[0;32m~/anaconda3/envs/refact3/lib/python3.9/json/__init__.py:346\u001b[0m, in \u001b[0;36mloads\u001b[0;34m(s, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[1;32m    341\u001b[0m     s \u001b[38;5;241m=\u001b[39m s\u001b[38;5;241m.\u001b[39mdecode(detect_encoding(s), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msurrogatepass\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    343\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m object_hook \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[1;32m    344\u001b[0m         parse_int \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m parse_float \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[1;32m    345\u001b[0m         parse_constant \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m object_pairs_hook \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m kw):\n\u001b[0;32m--> 346\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_default_decoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    347\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    348\u001b[0m     \u001b[38;5;28mcls\u001b[39m \u001b[38;5;241m=\u001b[39m JSONDecoder\n",
      "File \u001b[0;32m~/anaconda3/envs/refact3/lib/python3.9/json/decoder.py:337\u001b[0m, in \u001b[0;36mJSONDecoder.decode\u001b[0;34m(self, s, _w)\u001b[0m\n\u001b[1;32m    332\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdecode\u001b[39m(\u001b[38;5;28mself\u001b[39m, s, _w\u001b[38;5;241m=\u001b[39mWHITESPACE\u001b[38;5;241m.\u001b[39mmatch):\n\u001b[1;32m    333\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Return the Python representation of ``s`` (a ``str`` instance\u001b[39;00m\n\u001b[1;32m    334\u001b[0m \u001b[38;5;124;03m    containing a JSON document).\u001b[39;00m\n\u001b[1;32m    335\u001b[0m \n\u001b[1;32m    336\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 337\u001b[0m     obj, end \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraw_decode\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_w\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mend\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    338\u001b[0m     end \u001b[38;5;241m=\u001b[39m _w(s, end)\u001b[38;5;241m.\u001b[39mend()\n\u001b[1;32m    339\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m end \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(s):\n",
      "File \u001b[0;32m~/anaconda3/envs/refact3/lib/python3.9/json/decoder.py:355\u001b[0m, in \u001b[0;36mJSONDecoder.raw_decode\u001b[0;34m(self, s, idx)\u001b[0m\n\u001b[1;32m    353\u001b[0m     obj, end \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscan_once(s, idx)\n\u001b[1;32m    354\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[0;32m--> 355\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m JSONDecodeError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpecting value\u001b[39m\u001b[38;5;124m\"\u001b[39m, s, err\u001b[38;5;241m.\u001b[39mvalue) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    356\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m obj, end\n",
      "\u001b[0;31mJSONDecodeError\u001b[0m: Expecting value: line 1 column 2 (char 1)"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# Read and process line by line (memory efficient for large files)\n",
    "with open('comprehensive_evaluation_prompts.jsonl', 'r') as file:\n",
    "    for line in file:\n",
    "        data = json.loads(line.strip())\n",
    "        #print(data['prompt'])\n",
    "\n",
    "        output = ollama.generate(\n",
    "          model=\"starcoder2_20250704:latest\",\n",
    "          prompt=data['prompt']\n",
    "        )   \n",
    "        print(output['response'])\n",
    "        print('=============================================')\n",
    "        # Process each JSON object here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e4ebeba5-7053-461a-9c13-d6d434cb0cc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "//\n",
      "// Created by Aleksey Ovod on 25/10/16.\n",
      "// Copyright (c) 2016 Orc. All rights reserved.\n",
      "//\n",
      "\n",
      "#ifndef TBRICKS_APPS_VOLATILITY_MANAGER_SHARED_VOLATILITY_CURVE_MANAGER_H\n",
      "#define TBRICKS_APPS_VOLATILITY_MANAGER_SHARED_VOLATILITY_CURVE_MANAGER_H\n",
      "\n",
      "#include \"IVolatilityCurveManager.h\"\n",
      "#include <shared/Macros.h>\n",
      "SUPPRESS_SF_API_CLANG_WARNINGS\n",
      "#include <strategy/type/Uuid.h>\n",
      "#include <strategy/type/Integer.h>\n",
      "#include <strategy/type/Double.h>\n",
      "#include <strategy/type/String.h>\n",
      "#include <strategy/type/Table.h>\n",
      "CLANG_REST\n",
      "=============================================\n",
      "\n",
      "\n",
      "=============================================\n",
      "\n",
      "// Copyright 2016 Itiviti Group All rights reserved.\n",
      "// Reproduction in whole or in part in any form or medium without express\n",
      "// written permission of Itiviti Group is strictly prohibited.\n",
      "//\n",
      "// This is an automated-generated file. Please do not edit it manually.\n",
      "// Use Front-end to edit metadata and \"tbplugin export\" instead.\n",
      "//\n",
      "#ifndef __TBRICKS__VOLATILITY_MANAGER__DEFINITIONS_H__\n",
      "#define __TBRICKS__VOLATILITY_MANAGER__DEFINITIONS_H__\n",
      "\n",
      "#include \"strategy/Types.h\"\n",
      "#include \"strategy/Linkage.h\"\n",
      "#include \"strategy/pricing/InstrumentCalculatedValueDefinition.h\"\n",
      "#include \"strategy/calculated_property/CalculatedPropertyDefinition.h\"\n",
      "\n",
      "namespace volatility_manager {\n",
      "\n",
      "namespace plug_ins {\n",
      "\n",
      "// Plug-in \"Vol\n",
      "=============================================\n",
      "\n",
      "\n",
      "=============================================\n",
      "\n",
      "\n",
      "=============================================\n",
      "\n",
      "=============================================\n",
      "\n",
      "\n",
      "=============================================\n",
      "\n",
      "=============================================\n",
      "\n",
      "=============================================\n",
      "\n",
      "\n",
      "=============================================\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd \n",
    "#Reading the JSON File \n",
    "dataFrame = pd.read_json(\"fim_prompts_from_codebase.json\")\n",
    "\n",
    "for ix,row in dataFrame.iterrows():\n",
    "    \n",
    "    output = ollama.generate(\n",
    "      model=\"starcoder2_20250704:latest\",\n",
    "      prompt=row['prompt']\n",
    "    )  \n",
    "    print(output['response'])\n",
    "    print('=============================================')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fd5c5a3f-7840-4e71-b70f-2a17e8d7e3b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d69be01f25f74b70b29acdcd1e2fa442",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# load the base and fine-tuned model\n",
    "import argparse\n",
    "import multiprocessing\n",
    "import os\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "import torch\n",
    "import transformers\n",
    "from accelerate import PartialState\n",
    "from datasets import load_dataset\n",
    "from peft import LoraConfig\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    BitsAndBytesConfig,\n",
    "    logging,\n",
    "    set_seed,\n",
    ")\n",
    "from trl import SFTTrainer\n",
    "from peft import PeftModel, PeftConfig\n",
    "\n",
    "\n",
    "nf4_config = BitsAndBytesConfig(\n",
    "   load_in_4bit=True,\n",
    "   bnb_4bit_quant_type=\"nf4\",\n",
    "   bnb_4bit_use_double_quant=True,\n",
    "   bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "config = PeftConfig.from_pretrained(\"/usr/project/starcoder2/finetune_starcoder2/checkpoint-10000\")\n",
    "base_model = \"bigcode/starcoder2-7b\"\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model,\n",
    "    #quantization_config=nf4_config,\n",
    "    device_map=\"auto\",\n",
    "    offload_folder=\"offload\",\n",
    "    offload_state_dict=True, \n",
    "    torch_dtype=torch.float16\n",
    ")\n",
    "model = PeftModel.from_pretrained(model, \"/usr/project/starcoder2/finetune_starcoder2/checkpoint-10000\")\n",
    "model = model.merge_and_unload()\n",
    "\n",
    "merged_model_path= f\"starcoder2_7b_22k_ft_80EM\"\n",
    "model.save_pretrained(merged_model_path)\n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"/usr/project/starcoder2/finetune_starcoder2/checkpoint-10000\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "97d8baa4-c215-4076-bba1-599b517d0416",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('starcoder2_7b_22k_ft_80EM/tokenizer_config.json',\n",
       " 'starcoder2_7b_22k_ft_80EM/special_tokens_map.json',\n",
       " 'starcoder2_7b_22k_ft_80EM/vocab.json',\n",
       " 'starcoder2_7b_22k_ft_80EM/merges.txt',\n",
       " 'starcoder2_7b_22k_ft_80EM/added_tokens.json',\n",
       " 'starcoder2_7b_22k_ft_80EM/tokenizer.json')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"/usr/project/starcoder2/finetune_starcoder2/checkpoint-10000\")\n",
    "tokenizer.save_pretrained(merged_model_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0f7d1227-ea55-42c4-89c7-c20a37019a86",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ffb48fc5dcd541788b8540d0ba634494",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# load the base and fine-tuned model\n",
    "import argparse\n",
    "import multiprocessing\n",
    "import os\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "import torch\n",
    "import transformers\n",
    "from accelerate import PartialState\n",
    "from datasets import load_dataset\n",
    "from peft import LoraConfig\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    BitsAndBytesConfig,\n",
    "    logging,\n",
    "    set_seed,\n",
    ")\n",
    "from trl import SFTTrainer\n",
    "from peft import PeftModel, PeftConfig\n",
    "# verify the load\n",
    "\n",
    "merged_model_path= f\"starcoder2_7b_22k_ft_80EM\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    merged_model_path,\n",
    "    device_map=\"cuda:0\",\n",
    "    torch_dtype=torch.float16     # if not specified, it might cast as f32 and cause OOM\n",
    "    # offload_folder=\"offload\",\n",
    "    # offload_state_dict=True,\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(merged_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "586ec980-815f-4391-9977-4c6b2d17eca6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d2bcb38523b48fba462af2a2cc21862",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# load the base and fine-tuned model\n",
    "import argparse\n",
    "import multiprocessing\n",
    "import os\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "import torch\n",
    "import transformers\n",
    "from accelerate import PartialState\n",
    "from datasets import load_dataset\n",
    "from peft import LoraConfig\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    BitsAndBytesConfig,\n",
    "    logging,\n",
    "    set_seed,\n",
    ")\n",
    "from trl import SFTTrainer\n",
    "from peft import PeftModel, PeftConfig\n",
    "\n",
    "\n",
    "model_file = \"/home/syu2/Downloads/tbqwen-3B/qwen2.5_coder_3b_base-lora-20250207-124005-iter0150-testloss0.620/\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_file,\n",
    "    device_map=\"cuda:0\",\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1df5e9f9-3a20-472b-b28e-59dced242b08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".\n",
      "#Please explain why you used it and how your implementation worked.\n",
      "\n",
      "import random\n",
      "from time import time\n",
      "\n",
      "def bubble_sort(items, start = 0, end = -1):\n",
      "    if end == -1:\n",
      "        end = len(items)\n",
      "\n",
      "    for i in range(start, end):\n",
      "        for j in range(end-1, i, -1):\n",
      "            if items[j] < items[j-1]:\n",
      "                items[j],\n"
     ]
    }
   ],
   "source": [
    "import ollama\n",
    "# generate a response combining the prompt and data we retrieved in step 2\n",
    "output = ollama.generate(\n",
    "  model=\"starcoder2EM80:latest\",\n",
    "  prompt=f\"Write me a sorting algorithm in python\"\n",
    ")\n",
    "\n",
    "print(output['response'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3e61a5bc-7ea4-448e-b151-34bb057cb836",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    if n <= 1:\n",
      "        return n\n",
      "<file_sep>/README.md\n",
      "# Introduction to Algorithms with Python\n",
      "\n",
      "## Overview\n",
      "\n",
      "This is a personal attempt to learn about the most common algorithms used in computer science. I will implement them in Python and try to explain their working principles behind them. The goal of this project is not to become an expert but just to have a better understanding of what's happening under the hood.\n",
      "\n",
      "## Contents\n",
      "\n",
      "1. [Sorting](sorting/) - implementation of various sorting algorithms\n",
      "2. [Searching](searching/) - implementation of various searching algorithms\n",
      "3. [Graph Theory](graph_theory/) - implementation of graph theory data structures and algorithms such as BFS, DFS and shortest path finding\n",
      "4. [Recursion](recursion/) - simple examples of recursive functions and their implementations with memorization\n",
      "5. [Probability](probability/) - example of random number generation and probability calculations\n",
      "6. [Data Structures](data_\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import os.path\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "import ollama\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')  # Suppress all other warnings\n",
    "os.environ['TRANSFORMERS_VERBOSITY'] = 'error'  # Suppress transformer warnings\n",
    "base_model = \"bigcode/starcoder2-7b\"\n",
    "# checkpoint = '/home/syu2/Downloads/merge_starcoder2/'\n",
    "# checkpoint2 = '/home/syu2/Downloads/merged_models_starcoder2_7b_iter1100/'\n",
    "# checkpoint3 = '/home/syu2/Downloads/merged_models_starcoder2_7b_iter1100/'\n",
    "# checkpoint4 = '/usr/project/starcoder2/finetune_starcoder2/final_checkpoint'\n",
    "\n",
    "# bnb_config = BitsAndBytesConfig(\n",
    "# load_in_4bit=True\n",
    "# )\n",
    "\n",
    "# model = AutoModelForCausalLM.from_pretrained(base_model,\n",
    "#                                   quantization_config=bnb_config,  \n",
    "#                                   device_map=\"cuda:0\"           \n",
    "#                                             )\n",
    "\n",
    "\n",
    "# # model = AutoModelForCausalLM.from_pretrained(\n",
    "# #     checkpoint, \n",
    "# #     #quantization_config=quantization_config,\n",
    "# #     cache_dir=MODEL_CACHE_DIR,\n",
    "# # )\n",
    "#tokenizer = AutoTokenizer.from_pretrained(base_model)\n",
    "#tokenizer.set_pad_token = 'PAD'\n",
    "\n",
    "def generate_completion(text: str):\n",
    "    inputs = tokenizer.encode(text, return_tensors=\"pt\").to(\"cuda:0\")\n",
    "    #print(len(inputs[0]))\n",
    "    outputs = model.generate(inputs, max_length=512, pad_token_id=tokenizer.eos_token_id)\n",
    "    return tokenizer.decode(outputs[0])\n",
    "\n",
    "def generate_completion_ollama(text: str):\n",
    "    output = ollama.generate(\n",
    "      model=\"starcoder2EM80_cleanf16:latest\",\n",
    "      prompt=text,\n",
    "    )    \n",
    "    return output['response']\n",
    "    \n",
    "text = \"<fim-prefix>def fib(n):<fim-suffix>    else:\\n        return fib(n - 2) + fib(n - 1)<fim-middle>\"\n",
    "text_underscore= \"<fim_prefix>def fib(n):<fim_suffix>    else:\\n        return fib(n - 2) + fib(n - 1)<fim_middle><|endoftext|>\"\n",
    "#text_qwen = \"<|fim_prefix|>def fib(n):<|fim_suffix|>   else:\\n        return fib(n - 2) + fib(n - 1)<|fim_middle|>\"\n",
    "text_qwen = \"<|fim_prefix|>def fib(n):<|fim_suffix|>   else:\\n        return fib(n - 2) + fib(n - 1)<|fim_middle|><|endoftext|>\"\n",
    "print(generate_completion_ollama(text_underscore))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46da34f8-9b5c-4399-bcc3-2ee9f0f0ba0c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd \n",
    "import re\n",
    "\n",
    "#Reading the JSON File \n",
    "#dataFrame = pd.read_json(\"fim_prompts_from_codebase.json\")\n",
    "#df = pd.read_json(\"prompts_origin_starcoder2_format.json\")\n",
    "#df = pd.read_json(\"fim_prompts_from_codebase.json\")\n",
    "#df = pd.read_json(\"fim_prompts_outputs_pandas_ready_codebase_new.jsonl\", lines=True)\n",
    "df = pd.read_json(\"tbricksnext2lines_train.jsonl\", lines=True)\n",
    "\n",
    "\n",
    "def extract_between_strings_inclusive_regex(text, start_pattern, end_pattern):\n",
    "    # Construct the regex pattern to capture the delimiters and the content\n",
    "    # re.escape() is used to treat the delimiters as literal strings\n",
    "    # (.*?) captures any characters non-greedily between the delimiters\n",
    "    pattern = re.escape(start_pattern) + r\"(.*?)\" + re.escape(end_pattern)\n",
    "    match = re.search(pattern, text, re.DOTALL)\n",
    "    if match:\n",
    "        # group(0) returns the entire matched string, including the delimiters\n",
    "        return match.group(0)\n",
    "    return None\n",
    "\n",
    "    \n",
    "count = 0\n",
    "for ix, row in df.iterrows():\n",
    "    count+=1\n",
    "    if count%100==0: \n",
    "        print(count)\n",
    "        df.to_json('tbricksnext2lines_train.jsonl_iter4k_completions_6thtry.json', orient='records', lines=True)\n",
    "    try:\n",
    "        real_prompt = extract_between_strings_inclusive_regex(row['content'],\"<fim-prefix>\", \"<fim-middle>\")\n",
    "        answers = generate_completion(real_prompt)\n",
    "        df.at[ix,'completion']= answers\n",
    "        #print('-------------Prediction ----------')\n",
    "        #print(real_prompt)\n",
    "        #print('********************')\n",
    "        #print(answers)\n",
    "        # print('-------------Ground Truth ----------')\n",
    "        # print(row['output'])\n",
    "        # print(\"==============================================\")\n",
    "\n",
    "    except Exception as err:\n",
    "        print(err)\n",
    "        df.at[ix,'completion'] = None\n",
    "        continue\n",
    "df.to_json('tbricksnext2lines_train.jsonl_iter4k_completions_2ntry.json', orient='records', lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "88e7f0aa-75aa-45b5-b8c6-dc561b96516e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "10\n",
      "20\n",
      "30\n",
      "40\n",
      "50\n",
      "60\n",
      "70\n",
      "80\n",
      "90\n",
      "100\n",
      "110\n",
      "120\n",
      "130\n",
      "140\n",
      "150\n",
      "160\n",
      "170\n",
      "180\n",
      "190\n",
      "200\n",
      "0\n",
      "10\n",
      "20\n",
      "30\n",
      "40\n",
      "50\n",
      "60\n",
      "70\n",
      "80\n",
      "90\n",
      "100\n",
      "110\n",
      "120\n",
      "130\n",
      "140\n",
      "150\n",
      "160\n",
      "170\n",
      "180\n",
      "190\n",
      "200\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import re\n",
    "\n",
    "def extract_between_strings_inclusive_regex(text, start_pattern, end_pattern):\n",
    "    # Construct the regex pattern to capture the delimiters and the content\n",
    "    # re.escape() is used to treat the delimiters as literal strings\n",
    "    # (.*?) captures any characters non-greedily between the delimiters\n",
    "    pattern = re.escape(start_pattern) + r\"(.*?)\" + re.escape(end_pattern)\n",
    "    match = re.search(pattern, text, re.DOTALL)\n",
    "    if match:\n",
    "        # group(0) returns the entire matched string, including the delimiters\n",
    "        return match.group(0)\n",
    "    return None\n",
    "\n",
    "\n",
    "icount = 0\n",
    "\n",
    "MODE = 'starcoder2'\n",
    "\n",
    "\n",
    "with open(\"prompt_nosuffix_train_17th.jsonl\", \"w\") as outfile:\n",
    "    \n",
    "    with open(\"prompt_nosuffix_train_fim_underscore_format.jsonl\", 'r', encoding='utf-8') as f:\n",
    "\n",
    "#    data = json.load(f)  # Load the whole file as a list\n",
    "    \n",
    "        for line in f:\n",
    "\n",
    "            \n",
    "            entry = json.loads(line)\n",
    "            \n",
    "            if icount>=201: break\n",
    "            if icount%10==0: print(icount)\n",
    "\n",
    "            try:\n",
    "                #print(entry['content'])\n",
    "                #real_prompt = extract_between_strings_inclusive_regex(entry['content'],\"<fim-prefix>\", \"<fim-middle>\")\n",
    "                real_prompt = extract_between_strings_inclusive_regex(entry['content'],\"<fim_prefix>\", \"<fim_middle>\")\n",
    "                #print(real_prompt)\n",
    "                if MODE=='qwen':\n",
    "                    real_prompt = real_prompt.replace(\"<FIM-PREFIX>\", \"<|fim_prefix|>\") \n",
    "                    #real_prompt = re.sub(\"<FIM-PREFIX>\", \"<|fim_prefix|>\", real_prompt, flags=re.IGNORECASE) \n",
    "                    real_prompt = real_prompt.replace(\"<FIM-MIDDLE>\", \"<|fim_middle|>\") \n",
    "                    \n",
    "                answers = generate_completion_ollama(real_prompt)  \n",
    "                \n",
    "                if MODE=='qwen':\n",
    "                    answers = answers.replace(\"<|fim_prefix|>\", \"<FIM-PREFIX>\")\n",
    "                    answers = answers.replace(\"<|fim_suffix|>\", \"<FIM-SUFFIX>\") \n",
    "                    answers = answers.replace(\"<|fim_middle|>\", \"<FIM-MIDDLE>\") \n",
    "                    \n",
    "                entry['completions'] = answers\n",
    "                \n",
    "                # if icount<10:\n",
    "                #     print('==============================')\n",
    "                #     print(real_prompt)\n",
    "                #     print('------------------------------')\n",
    "                #     print(answers)\n",
    "\n",
    "                    \n",
    "                if isinstance(entry, dict):\n",
    "                    outfile.write(json.dumps(entry, separators=(',', ':')))\n",
    "                    outfile.write(\"\\n\")\n",
    "                else:\n",
    "                    print(\"Skipped non-dict entry:\", entry)                \n",
    "\n",
    "                #alljson.append(entry)\n",
    "                icount+=1\n",
    "            except Exception as err:\n",
    "                print(err)\n",
    "                continue\n",
    "\n",
    "\n",
    "\n",
    "icount = 0\n",
    "\n",
    "\n",
    "with open(\"prompt_nosuffix_test_17th.jsonl\", \"w\") as outfile:\n",
    "    \n",
    "    with open(\"prompt_nosuffix_test_fim_underscore_format.jsonl\", 'r', encoding='utf-8') as f:\n",
    "\n",
    "#    data = json.load(f)  # Load the whole file as a list\n",
    "    \n",
    "        for line in f:\n",
    "\n",
    "            \n",
    "            entry = json.loads(line)\n",
    "            \n",
    "            if icount>=201: break\n",
    "            if icount%10==0: print(icount)\n",
    "\n",
    "            try:\n",
    "                #print(entry['content'])\n",
    "                #real_prompt = extract_between_strings_inclusive_regex(entry['content'],\"<fim-prefix>\", \"<fim-middle>\")\n",
    "                real_prompt = extract_between_strings_inclusive_regex(entry['content'],\"<fim_prefix>\", \"<fim_middle>\")\n",
    "                #print(real_prompt)\n",
    "                if MODE=='qwen':\n",
    "                    real_prompt = real_prompt.replace(\"<FIM-PREFIX>\", \"<|fim_prefix|>\") \n",
    "                    #real_prompt = re.sub(\"<FIM-PREFIX>\", \"<|fim_prefix|>\", real_prompt, flags=re.IGNORECASE) \n",
    "                    real_prompt = real_prompt.replace(\"<FIM-MIDDLE>\", \"<|fim_middle|>\") \n",
    "                    \n",
    "                answers = generate_completion_ollama(real_prompt)  \n",
    "                \n",
    "                if MODE=='qwen':\n",
    "                    answers = answers.replace(\"<|fim_prefix|>\", \"<FIM-PREFIX>\")\n",
    "                    answers = answers.replace(\"<|fim_suffix|>\", \"<FIM-SUFFIX>\") \n",
    "                    answers = answers.replace(\"<|fim_middle|>\", \"<FIM-MIDDLE>\") \n",
    "                    \n",
    "                entry['completions'] = answers\n",
    "                \n",
    "                # if icount<10:\n",
    "                #     print('==============================')\n",
    "                #     print(real_prompt)\n",
    "                #     print('------------------------------')\n",
    "                #     print(answers)\n",
    "\n",
    "                    \n",
    "                if isinstance(entry, dict):\n",
    "                    outfile.write(json.dumps(entry, separators=(',', ':')))\n",
    "                    outfile.write(\"\\n\")\n",
    "                else:\n",
    "                    print(\"Skipped non-dict entry:\", entry)                \n",
    "\n",
    "                #alljson.append(entry)\n",
    "                icount+=1\n",
    "            except Exception as err:\n",
    "                print(err)\n",
    "                continue\n",
    "\n",
    "\n",
    "# icount = 0\n",
    "\n",
    "# with open(\"tbricksnext2lines_test.jsonl\", 'r', encoding='utf-8') as f:\n",
    "\n",
    "# #with open(\"tbricksnext2lines_iter4k_completions_2ntry.jsonl\", 'r', encoding='utf-8') as f:    \n",
    "#     # Writing to sample.json\n",
    "#     with open(\"tbricksnext2lines_test_completions_qwen3Bbest.jsonl\", \"w\") as outfile:    \n",
    "#         for line in f:\n",
    "    \n",
    "#             entry = json.loads(line)\n",
    "#             icount+=1\n",
    "#             if icount>=201: break\n",
    "#             if icount%10==0: print(icount)\n",
    "                \n",
    "#             try:\n",
    "#                 real_prompt = extract_between_strings_inclusive_regex(entry['content'],\"<fim-prefix>\", \"<fim-middle>\")\n",
    "\n",
    "#                 if MODE=='qwen':\n",
    "#                     real_prompt = real_prompt.replace(\"<FIM-PREFIX>\", \"<|fim_prefix|>\") \n",
    "#                     #real_prompt = re.sub(\"<FIM-PREFIX>\", \"<|fim_prefix|>\", real_prompt, flags=re.IGNORECASE) \n",
    "#                     real_prompt = real_prompt.replace(\"<FIM-MIDDLE>\", \"<|fim_middle|>\") \n",
    "                    \n",
    "#                 answers = generate_completion(real_prompt)  \n",
    "\n",
    "#                 if MODE=='qwen':\n",
    "#                     answers = answers.replace(\"<|fim_prefix|>\", \"<FIM-PREFIX>\")\n",
    "#                     answers = answers.replace(\"<|fim_suffix|>\", \"<FIM-SUFFIX>\") \n",
    "#                     answers = answers.replace(\"<|fim_middle|>\", \"<FIM-MIDDLE>\") \n",
    "                    \n",
    "#                 entry['completion'] = answers\n",
    "#                 if icount<10: print(answers)\n",
    "#                 if isinstance(entry, dict):\n",
    "#                     outfile.write(json.dumps(entry, separators=(',', ':')))\n",
    "#                     outfile.write(\"\\n\")\n",
    "#                 else:\n",
    "#                     print(\"Skipped non-dict entry:\", entry)                \n",
    "\n",
    "#                 #alljson.append(entry)\n",
    "#             except Exception as err:\n",
    "#                 print(err)\n",
    "#                 continue\n",
    "            \n",
    "\n",
    "# icount = 0\n",
    "\n",
    "# with open(\"tbricksnext2lines_train.jsonl\", 'r', encoding='utf-8') as f:\n",
    "\n",
    "# #with open(\"tbricksnext2lines_iter4k_completions_2ntry.jsonl\", 'r', encoding='utf-8') as f:    \n",
    "#     # Writing to sample.json\n",
    "#     with open(\"tbricksnext2lines_train_completions_qwen3Bbest.jsonl\", \"w\") as outfile:    \n",
    "#         for line in f:\n",
    "    \n",
    "#             entry = json.loads(line)\n",
    "#             icount+=1\n",
    "#             if icount>=201: break\n",
    "#             if icount%10==0: print(icount)\n",
    "                \n",
    "#             try:\n",
    "#                 real_prompt = extract_between_strings_inclusive_regex(entry['content'],\"<fim-prefix>\", \"<fim-middle>\")\n",
    "\n",
    "#                 if MODE=='qwen':\n",
    "#                     real_prompt = real_prompt.replace(\"<FIM-PREFIX>\", \"<|fim_prefix|>\") \n",
    "#                     #real_prompt = re.sub(\"<FIM-PREFIX>\", \"<|fim_prefix|>\", real_prompt, flags=re.IGNORECASE) \n",
    "#                     real_prompt = real_prompt.replace(\"<FIM-MIDDLE>\", \"<|fim_middle|>\") \n",
    "                    \n",
    "#                 answers = generate_completion(real_prompt)  \n",
    "\n",
    "#                 if MODE=='qwen':\n",
    "#                     answers = answers.replace(\"<|fim_prefix|>\", \"<FIM-PREFIX>\")\n",
    "#                     answers = answers.replace(\"<|fim_suffix|>\", \"<FIM-SUFFIX>\") \n",
    "#                     answers = answers.replace(\"<|fim_middle|>\", \"<FIM-MIDDLE>\") \n",
    "                \n",
    "#                 entry['completion'] = answers\n",
    "#                 if icount<10: print(answers)\n",
    "#                 if isinstance(entry, dict):\n",
    "#                     outfile.write(json.dumps(entry, separators=(',', ':')))\n",
    "#                     outfile.write(\"\\n\")\n",
    "#                 else:\n",
    "#                     print(\"Skipped non-dict entry:\", entry)                \n",
    "\n",
    "#                 #alljson.append(entry)\n",
    "#             except Exception as err:\n",
    "#                 print(err)\n",
    "#                 continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ac7ecb22-59b4-4a58-80fc-0c8e3206a8a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "201"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "icount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7d2cf727-b1f1-47cb-9fb3-54830e380de7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<fim-prefix> class IBasketModuleHandler : public Module::IModuleHandler { public: virtual void BasketHedgerModule_StatisticsChanged(BasketHedgerModule& sender) = 0;<fim-suffix> }; BasketHedgerModule(BasketHedgerModule::IBasketModuleHandler& handler);<fim-middle>\n",
      "\n",
      "#include \"shared/API.h\"\n",
      "\n",
      "using namespace tbricks;\n",
      "\n",
      "class BasketHedgerModule : public Module, public IRequestReplyHandler, public ITimerEventHandler, public IBestPriceStream::IHandler\n"
     ]
    }
   ],
   "source": [
    "text3 = \"<fim-prefix> class IBasketModuleHandler : public Module::IModuleHandler { public: virtual void BasketHedgerModule_StatisticsChanged(BasketHedgerModule& sender) = 0;<fim-suffix> }; BasketHedgerModule(BasketHedgerModule::IBasketModuleHandler& handler);<fim-middle>\"\n",
    "print(generate_completion(text3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "997209ca-44a3-4aa8-8aa1-ff243d362466",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<fim-prefix> virtual void HedgerModule_CompletelyMissedHedge(HedgerModule& sender, const Instrument& missingHedgeInstrument, const String& reason) { };<fim-suffix> virtual void HedgerModule_Failed(HedgerModule& sender, const String& reason) {} virtual void HedgerModule_Recovered(HedgerModule& sender) {}<fim-middle>\n",
      "\n",
      "class HedgerModule : public Strategy, public IRequestReplyHandler, public BestPriceStream::IHandler, public StatisticsStream::IHandler, public\n"
     ]
    }
   ],
   "source": [
    "text4 = \"<fim-prefix> virtual void HedgerModule_CompletelyMissedHedge(HedgerModule& sender, const Instrument& missingHedgeInstrument, const String& reason) { };<fim-suffix> virtual void HedgerModule_Failed(HedgerModule& sender, const String& reason) {} virtual void HedgerModule_Recovered(HedgerModule& sender) {}<fim-middle>\"\n",
    "print(generate_completion(text4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f71c906b-b0b1-4bab-995a-051c8bc117b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "text5 = \"<fim-prefix> virtual void HedgerModule_CompletelyMissedHedge(HedgerModule& sender, const Instrument& missingHedgeInstrument, const String& reason) { };<fim-suffix> virtual void HedgerModule_Failed(HedgerModule& sender, const String& reason) {} virtual void HedgerModule_Recovered(HedgerModule& sender) {}<fim-middle>\"\n",
    "print(generate_completion(text4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "983552c8-4355-40a1-af4a-f5c0469db7ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_json(\"fim_prompts_from_codebase.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e00f8025-49b3-4b7d-878b-27841430c217",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prompt</th>\n",
       "      <th>output</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>&lt;fim-prefix&gt;    class IHandler\\n    {\\n    pub...</td>\n",
       "      <td>\\n        virtual void HandleDriverInstrumentB...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>&lt;fim-prefix&gt;  class IBasketModuleHandler : pub...</td>\n",
       "      <td>\\n    virtual void BasketHedgerModule_Complete...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>&lt;fim-prefix&gt;    virtual void Clear() override;...</td>\n",
       "      <td>\\n\\npublic:\\n    virtual void GetVolatilityPar...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>&lt;fim-prefix&gt;    virtual void HedgerModule_Comp...</td>\n",
       "      <td>\\n\\n    virtual void HedgerModule_Traded(Hedge...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>&lt;fim-prefix&gt;public:\\n\\tvirtual void BasePriceU...</td>\n",
       "      <td>\\n\\tvirtual void BaseBBOUpdate( const BaseInfo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>&lt;fim-prefix&gt;    const IVolatilityModel * GetLe...</td>\n",
       "      <td>\\n    void ClearLeftModel() { delete m_left; m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>&lt;fim-prefix&gt;    virtual void HandleRunRequest(...</td>\n",
       "      <td>\\n    virtual void HandleModifyRequest(const t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>&lt;fim-prefix&gt;    enum BookoutInputState\\n    {\\...</td>\n",
       "      <td>\\n        , INVALID_TRADE_VOLUME</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>&lt;fim-prefix&gt;    const InstrumentGroupIdentifie...</td>\n",
       "      <td>\\n    void SetMaturityGroupId(const Instrument...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>&lt;fim-prefix&gt;    void PostCall(std::function&lt;vo...</td>\n",
       "      <td>\\n\\n    bool IsDirty() const { return m_dirtyC...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              prompt  \\\n",
       "0  <fim-prefix>    class IHandler\\n    {\\n    pub...   \n",
       "1  <fim-prefix>  class IBasketModuleHandler : pub...   \n",
       "2  <fim-prefix>    virtual void Clear() override;...   \n",
       "3  <fim-prefix>    virtual void HedgerModule_Comp...   \n",
       "4  <fim-prefix>public:\\n\\tvirtual void BasePriceU...   \n",
       "5  <fim-prefix>    const IVolatilityModel * GetLe...   \n",
       "6  <fim-prefix>    virtual void HandleRunRequest(...   \n",
       "7  <fim-prefix>    enum BookoutInputState\\n    {\\...   \n",
       "8  <fim-prefix>    const InstrumentGroupIdentifie...   \n",
       "9  <fim-prefix>    void PostCall(std::function<vo...   \n",
       "\n",
       "                                              output  \n",
       "0  \\n        virtual void HandleDriverInstrumentB...  \n",
       "1  \\n    virtual void BasketHedgerModule_Complete...  \n",
       "2  \\n\\npublic:\\n    virtual void GetVolatilityPar...  \n",
       "3  \\n\\n    virtual void HedgerModule_Traded(Hedge...  \n",
       "4  \\n\\tvirtual void BaseBBOUpdate( const BaseInfo...  \n",
       "5  \\n    void ClearLeftModel() { delete m_left; m...  \n",
       "6  \\n    virtual void HandleModifyRequest(const t...  \n",
       "7                   \\n        , INVALID_TRADE_VOLUME  \n",
       "8  \\n    void SetMaturityGroupId(const Instrument...  \n",
       "9  \\n\\n    bool IsDirty() const { return m_dirtyC...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4c98b663-f380-4d19-94f8-c052f5997971",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[target string]\n",
      "<fim-prefix>    }\n",
      "\n",
      "    if (m_values[spot_price].size() > 2) {\n",
      "        int last = m_values[spot_price].size() - 1;\n",
      "        int penultimate = last - 1;<fim-suffix> <fim-middle>\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def extract_between_strings_inclusive_regex(text, start_pattern, end_pattern):\n",
    "    # Construct the regex pattern to capture the delimiters and the content\n",
    "    # re.escape() is used to treat the delimiters as literal strings\n",
    "    # (.*?) captures any characters non-greedily between the delimiters\n",
    "    pattern = re.escape(start_pattern) + r\"(.*?)\" + re.escape(end_pattern)\n",
    "    match = re.search(pattern, text, re.DOTALL)\n",
    "    if match:\n",
    "        # group(0) returns the entire matched string, including the delimiters\n",
    "        return match.group(0)\n",
    "    return None\n",
    "\n",
    "# Example usage:\n",
    "my_string = \"This is a [target string] within the text.\"\n",
    "extracted = extract_between_strings_inclusive_regex(my_string, \"[\", \"]\")\n",
    "print(extracted)\n",
    "\n",
    "my_string2 =  '''<fim-prefix>    }\\n\\n    if (m_values[spot_price].size() > 2) {\\n        int last = m_values[spot_price].size() - 1;\\n        int penultimate = last - 1;<fim-suffix> <fim-middle>        grid_values[OptionModelCalculator::speed][0] = grid_values[OptionModelCalculator::speed][1];\\n        grid_values[OptionModelCalculator::speed][last] = grid_values[OptionModelCalculator::speed][penultimate];'''\n",
    "extracted_2= extract_between_strings_inclusive_regex(my_string2, \"<fim-prefix>\", \"<fim-middle>\")\n",
    "print(extracted_2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c8376f82-5d77-4bf2-b192-e0ba15c5fed2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|fim_prefix|>#include \"InstrumentEnricherPlugin.h\"\n",
      "#include \"tbricks_definitions.h\"\n",
      "<|fim_middle|>#include \"InstrumentEnricherPlugin.h\"\n",
      "\n",
      "namespace tbricks\n",
      "{\n",
      "\n",
      "<FIM-PREFIX>#include \"InstrumentEnricherPlugin.h\"\n",
      "#include \"tbricks_definitions.h\"\n",
      "<FIM-MIDDLE>#include \"InstrumentEnricherPlugin.h\"\n",
      "\n",
      "namespace tbricks\n",
      "{\n",
      "\n"
     ]
    }
   ],
   "source": [
    "s1 =\"\"\"<|fim_prefix|>#include \"InstrumentEnricherPlugin.h\"\n",
    "#include \"tbricks_definitions.h\"\n",
    "<|fim_middle|>#include \"InstrumentEnricherPlugin.h\"\n",
    "\n",
    "namespace tbricks\n",
    "{\n",
    "\"\"\"\n",
    "\n",
    "print(s1)\n",
    "\n",
    "#s2 = re.sub(\"<|fim_prefix|>\", \"<FIM-PREFIX>\", s1, count=1) \n",
    "s2 = s1.replace(\"<|fim_prefix|>\", \"<FIM-PREFIX>\")\n",
    "s2 = s2.replace(\"<|fim_suffix|>\", \"<FIM-SUFFIX>\") \n",
    "s2 = s2.replace(\"<|fim_middle|>\", \"<FIM-MIDDLE>\") \n",
    "\n",
    "print(s2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4fa324f6-e616-4a96-91d7-19fc30f6f8f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with open(\"prompt_nosuffix_test.jsonl\", 'r', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        \n",
    "        entry = json.loads(line)\n",
    "        print(entry['content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d83c16b1-41e8-4c3b-8ab8-767b79bda594",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<fim-prefix>#include <optional> #include \"DeribitMdp.h\" #include \"tbricks_definitions.h\" #define MAX_INSTRUMENTS 10 using namespace tbricks; DeribitMdp::DeribitMdp() { TBDEBUG(\"Constructor: \" << GetVenueIdentifier()); } void DeribitMdp::HandleSubscribe(MarketDataItem & item) { TBDEBUG(__func__ << \", item = \" << item); const auto & instrument_id = item.GetInstrumentVenueIdentification().GetInstrumentIdentifier(); auto model = DeribitModel(item, *this); m_models.emplace(instrument_id, model); m_streams.emplace(model.GetStreamId(), instrument_id); } void DeribitMdp::HandleUnsubscribe(MarketDataItem & item) { TBDEBUG(__func__ << \", item = \" << item); const auto & instrument_id = item.GetInstrumentVenueIdentification().GetInstrumentIdentifier(); auto model_opt = GetModel(instrument_id); if (model_opt) { auto * model = *model_opt; m_streams.erase(model->GetStreamId()); m_models.erase(instrument_id); } } void DeribitMdp::HandleOrderBookSubscribe(OrderBookItem & item) { TBDEBUG(__func__ << \", item = \" << item); } void DeribitMdp::HandleOrderBookUnsubscribe(OrderBookItem & item) { TBDEBUG(__func__ << \", item = \" << item); } bool DeribitMdp::IsMarketDataSupported(const InstrumentVenueIdentification & ivid) { TBDEBUG(__func__ << \", ivid = \" << ivid); return (m_models.size() < MAX_INSTRUMENTS); } void DeribitMdp::GetDiagnostics(Diagnostics & diagnostics) { diagnostics.GetDebugStream() << \"Market aggregator diagnostic \"; if (!diagnostics.GetDiagnosticKey().empty()) { { diagnostics.GetDebugStream() << \"for instrument \" << diagnostics.GetDiagnosticKey(); } } diagnostics.GetDebugStream() << std::endl; } void DeribitMdp::HandleStreamOpen(const StreamIdentifier & stream) { TBDEBUG(__func__ << \": \" << stream); } void DeribitMdp::HandleStreamStale(const StreamIdentifier & stream) { TBWARNING(__func__ << \": \" << stream); auto model_opt = GetModel(stream); if (model_opt) { auto * model = *model_opt; model->Reset(); } else { TBDEBUG(\"Unknown stream\"); } } void DeribitMdp::HandleStreamFailed(const StreamIdentifier & stream) { TBWARNING(__func__ << \": \"<FIM_SUFFIX>auto model_opt = GetModel(stream); if (model_opt) { auto * model = *model_opt; model->Fail(); } else { TBDEBUG(\"Unknown stream\"); } } void DeribitMdp::HandleSnapshotEnd(const StreamIdentifier & stream) { TBDEBUG(__func__ << \": \" << stream); } void DeribitMdp::HandleDistributedValues(const StreamIdentifier & stream, const DistributedValues::Update & update) { TBDEBUG(__func__ << \": \" << stream); TBDUMP(update); auto model_opt = GetModel(stream); if (model_opt) { auto * model = *model_opt; model->HandleDistributedValuesUpdate(update); } else { TBDEBUG(\"No Deribit model associated with stream: \" << stream); } } std::optional<DeribitModel *> DeribitMdp::GetModel(const StreamIdentifier & stream) {<fim-suffix><fim-middle><< stream);'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entry['content']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "77793bbe-476c-4a73-aca5-70e442ed90be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No choices found in the response.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "# Define the API endpoint\n",
    "url = \"http://localhost:11484/completion\"\n",
    "\n",
    "# Define the payload \n",
    "payload = {\n",
    "    \"model\": \"starcoder2_7b_22k_ft_80EM\",\n",
    "    \"prompt\": '''You are a helpful assistant<|im_end|>\n",
    "<|im_start|>user\n",
    "Hello<|im_end|>\n",
    "<|im_start|>assistant\n",
    "Hi there<|im_end|>\n",
    "<|im_start|>user\n",
    "How are you?<|im_end|>\n",
    "<|im_start|>assistant''',\n",
    "    \"temperature\": 0.7,\n",
    "    \"max_tokens\": 50\n",
    "}\n",
    "\n",
    "headers = {\"Content-Type\": \"application/json\"}\n",
    "try:\n",
    "    response = requests.post(url, json=payload, headers=headers)\n",
    "\n",
    "    # Check if the request was successful\n",
    "    if response.status_code == 200:\n",
    "        # Parse the response JSON\n",
    "        response_data = response.json()\n",
    "\n",
    "        # Extract the result from the response\n",
    "        choices = response_data.get(\"choices\", [])\n",
    "        if choices:\n",
    "            result = choices[0].get(\"text\", \"\")\n",
    "            print(\"Response:\", result)\n",
    "        else:\n",
    "            print(\"No choices found in the response.\")\n",
    "    else:\n",
    "        print(f\"Request failed with status code {response.status_code}: {response.text}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a608fe91-608a-41de-858d-57fab06e85f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "Completion results:\n",
      "Completion(id='cmpl-9bc80760ddd54d46ad06e4692854c7fa', choices=[CompletionChoice(finish_reason='length', index=0, logprobs=Logprobs(text_offset=[0, 5, 8, 12, 20, 26, 31, 34, 40, 44, 46, 49, 53, 56, 59, 61], token_logprobs=[-11.179304122924805, -1.6712048053741455, -0.680696427822113, -9.825337409973145, -5.072486877441406, -11.161785125732422, -4.6182050704956055, -9.84610366821289, -5.498493194580078, -8.561087608337402, -1.0149319171905518, -7.370491981506348, -0.09409049153327942, -0.7393490672111511, -0.41492289304733276, -3.5969433784484863], tokens=['Check', 'Ġif', 'Ġthe', 'Ġspecial', 'Ġblock', 'Ġself', '-is', 'Ġcross', 'pond', '.Ċ', 'ĠĠĠ', 'Ġ*/Ċ', 'ĠĠĠ', 'Ġif', 'Ġ(', 'side'], top_logprobs=[{'Check': -11.179304122924805, 'from': -1.1168044805526733, '#': -1.8668044805526733, 'class': -2.116804599761963}, {'Ġif': -1.6712048053741455, 'Ġthe': -2.8587048053741455, 'Ġthat': -3.2337048053741455}, {'Ġthe': -0.680696427822113, 'Ġa': -2.805696487426758, 'Ġtwo': -3.055696487426758}, {'Ġspecial': -9.825337409973145, 'Ġgiven': -1.3253370523452759, 'Ġinput': -2.5128369331359863, 'Ġnumber': -2.6378369331359863}, {'Ġblock': -5.072486877441406, 'Ġproperty': -2.2599868774414062, 'Ġcode': -2.5724868774414062, 'Ġkey': -3.0724868774414062}, {'Ġself': -11.161785125732422, 'Ġis': -0.7555348873138428, 'Ġexists': -2.8805348873138428, 'Ġof': -3.0055348873138428}, {'-is': -4.6182050704956055, 'Ġis': -2.8057048320770264, '._': -2.8682048320770264, '-': -3.3682048320770264}, {'Ġcross': -9.84610366821289, 'olated': -1.439853549003601, 'Ġthe': -2.6898536682128906, 'ol': -3.1273536682128906}, {'pond': -5.498493194580078, 'listed': -2.373493194580078, 'Ċ': -3.060993194580078, 'Ġthe': -3.185993194580078}, {'.Ċ': -8.561087608337402, 'ing': -0.12358780950307846, 's': -3.4985878467559814, 'Ġto': -4.061087608337402}, {'ĠĠĠ': -1.0149319171905518, 'ĠĠĠĠĠĠĠ': -1.0149319171905518, 'ĠĠĠĠĠĠĠĠĠĠĠ': -2.6399319171905518}, {'Ġ*/Ċ': -7.370491981506348, 'Ġif': -0.7454918026924133, 'Ġreturn': -2.2454917430877686, 'Ġfor': -3.7454917430877686}, {'ĠĠĠ': -0.09409049153327942, 'ĠĠĠĠĊ': -4.219090461730957, 'ĠĠ': -4.781590461730957}, {'Ġif': -0.7393490672111511, 'Ġreturn': -2.176849126815796, 'Ġbool': -2.364349126815796}, {'Ġ(': -0.41492289304733276, 'Ġ(!': -1.8524229526519775, 'Ġ((': -2.6024229526519775}, {'side': -3.5969433784484863, 'm': -1.3469432592391968, 'p': -2.6594433784484863, 'Get': -3.1594433784484863}]), text='Check if the special block self-is crosspond.\\n    */\\n    if (side', stop_reason=None, prompt_logprobs=None), CompletionChoice(finish_reason='length', index=1, logprobs=Logprobs(text_offset=[0, 3, 5, 8, 15, 20, 28, 35, 36, 43, 51, 59, 62, 70, 81, 83], token_logprobs=[-4.762409210205078, -0.41076403856277466, -0.5941911935806274, -0.004673866089433432, -0.003037602873519063, -0.056132953613996506, -0.006603917572647333, -0.0001658063702052459, -0.0011628062929958105, -0.008314046077430248, -0.002553062280640006, -0.00031704644788987935, -0.010180451907217503, -0.004279265645891428, -0.012154560536146164, -0.0013250865740701556], tokens=['/*Ċ', 'Ġ*', 'ĠTo', 'Ġchange', 'Ġthis', 'Ġlicense', 'Ġheader', ',', 'Ġchoose', 'ĠLicense', 'ĠHeaders', 'Ġin', 'ĠProject', 'ĠProperties', '.Ċ', 'Ġ*'], top_logprobs=[{'/*Ċ': -4.762409210205078, 'from': -1.1374093294143677, '#': -1.8874093294143677, 'class': -2.012409210205078}, {'Ġ*': -0.41076403856277466, 'ĠĠĠ': -2.41076397895813, '*': -2.66076397895813}, {'ĠTo': -0.5941911935806274, 'ĠFile': -3.219191074371338, 'Ġ': -3.281691074371338}, {'Ġchange': -0.004673866089433432, 'Ġview': -8.504673957824707, 'Ġ': -8.629673957824707}, {'Ġthis': -0.003037602873519063, 'Ġ': -7.628037452697754, 'Ġt': -7.940537452697754}, {'Ġlicense': -0.056132953613996506, 'Ġtemplate': -2.9936330318450928, 'Ġlicens': -7.431132793426514}, {'Ġheader': -0.006603917572647333, 'Ġhe': -6.881603717803955, 'header': -7.069103717803955}, {',': -0.0001658063702052459, ',Ċ': -9.562665939331055, 'Ċ': -11.000165939331055}, {'Ġchoose': -0.0011628062929958105, 'Ġcho': -8.1261625289917, 'Ġ': -8.1261625289917}, {'ĠLicense': -0.008314046077430248, 'ĠL': -5.75831413269043, 'ĠLic': -5.88331413269043}, {'ĠHeaders': -0.002553062280640006, 'Headers': -6.8150529861450195, 'Ġ': -7.6900529861450195}, {'Ġin': -0.00031704644788987935, 'Ġi': -8.875316619873047, 'Ġ': -9.187816619873047}, {'ĠProject': -0.010180451907217503, 'ĠProj': -6.010180473327637, 'Ġ': -6.572680473327637}, {'ĠProperties': -0.004279265645891428, 'ĠSettings': -6.879279136657715, 'Ġ': -7.504279136657715}, {'.Ċ': -0.012154560536146164, '.': -4.574654579162598, 'Ċ': -6.887154579162598}, {'Ġ*': -0.0013250865740701556, 'Ġ*Ċ': -7.9388251304626465, 'Ġ': -8.126324653625488}]), text='/*\\n * To change this license header, choose License Headers in Project Properties.\\n *', stop_reason=None, prompt_logprobs=None)], created=1753541731, model='/home/syu2/Downloads/tbqwen-3B/qwen2.5_coder_3b_base-lora-20250207-124005-iter0150-testloss0.620', object='text_completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=32, prompt_tokens=26, total_tokens=58, completion_tokens_details=None, prompt_tokens_details=None))\n",
      "--------------------------------------------------\n",
      "('id', 'cmpl-9bc80760ddd54d46ad06e4692854c7fa')\n",
      "('choices', [CompletionChoice(finish_reason='length', index=0, logprobs=Logprobs(text_offset=[0, 5, 8, 12, 20, 26, 31, 34, 40, 44, 46, 49, 53, 56, 59, 61], token_logprobs=[-11.179304122924805, -1.6712048053741455, -0.680696427822113, -9.825337409973145, -5.072486877441406, -11.161785125732422, -4.6182050704956055, -9.84610366821289, -5.498493194580078, -8.561087608337402, -1.0149319171905518, -7.370491981506348, -0.09409049153327942, -0.7393490672111511, -0.41492289304733276, -3.5969433784484863], tokens=['Check', 'Ġif', 'Ġthe', 'Ġspecial', 'Ġblock', 'Ġself', '-is', 'Ġcross', 'pond', '.Ċ', 'ĠĠĠ', 'Ġ*/Ċ', 'ĠĠĠ', 'Ġif', 'Ġ(', 'side'], top_logprobs=[{'Check': -11.179304122924805, 'from': -1.1168044805526733, '#': -1.8668044805526733, 'class': -2.116804599761963}, {'Ġif': -1.6712048053741455, 'Ġthe': -2.8587048053741455, 'Ġthat': -3.2337048053741455}, {'Ġthe': -0.680696427822113, 'Ġa': -2.805696487426758, 'Ġtwo': -3.055696487426758}, {'Ġspecial': -9.825337409973145, 'Ġgiven': -1.3253370523452759, 'Ġinput': -2.5128369331359863, 'Ġnumber': -2.6378369331359863}, {'Ġblock': -5.072486877441406, 'Ġproperty': -2.2599868774414062, 'Ġcode': -2.5724868774414062, 'Ġkey': -3.0724868774414062}, {'Ġself': -11.161785125732422, 'Ġis': -0.7555348873138428, 'Ġexists': -2.8805348873138428, 'Ġof': -3.0055348873138428}, {'-is': -4.6182050704956055, 'Ġis': -2.8057048320770264, '._': -2.8682048320770264, '-': -3.3682048320770264}, {'Ġcross': -9.84610366821289, 'olated': -1.439853549003601, 'Ġthe': -2.6898536682128906, 'ol': -3.1273536682128906}, {'pond': -5.498493194580078, 'listed': -2.373493194580078, 'Ċ': -3.060993194580078, 'Ġthe': -3.185993194580078}, {'.Ċ': -8.561087608337402, 'ing': -0.12358780950307846, 's': -3.4985878467559814, 'Ġto': -4.061087608337402}, {'ĠĠĠ': -1.0149319171905518, 'ĠĠĠĠĠĠĠ': -1.0149319171905518, 'ĠĠĠĠĠĠĠĠĠĠĠ': -2.6399319171905518}, {'Ġ*/Ċ': -7.370491981506348, 'Ġif': -0.7454918026924133, 'Ġreturn': -2.2454917430877686, 'Ġfor': -3.7454917430877686}, {'ĠĠĠ': -0.09409049153327942, 'ĠĠĠĠĊ': -4.219090461730957, 'ĠĠ': -4.781590461730957}, {'Ġif': -0.7393490672111511, 'Ġreturn': -2.176849126815796, 'Ġbool': -2.364349126815796}, {'Ġ(': -0.41492289304733276, 'Ġ(!': -1.8524229526519775, 'Ġ((': -2.6024229526519775}, {'side': -3.5969433784484863, 'm': -1.3469432592391968, 'p': -2.6594433784484863, 'Get': -3.1594433784484863}]), text='Check if the special block self-is crosspond.\\n    */\\n    if (side', stop_reason=None, prompt_logprobs=None), CompletionChoice(finish_reason='length', index=1, logprobs=Logprobs(text_offset=[0, 3, 5, 8, 15, 20, 28, 35, 36, 43, 51, 59, 62, 70, 81, 83], token_logprobs=[-4.762409210205078, -0.41076403856277466, -0.5941911935806274, -0.004673866089433432, -0.003037602873519063, -0.056132953613996506, -0.006603917572647333, -0.0001658063702052459, -0.0011628062929958105, -0.008314046077430248, -0.002553062280640006, -0.00031704644788987935, -0.010180451907217503, -0.004279265645891428, -0.012154560536146164, -0.0013250865740701556], tokens=['/*Ċ', 'Ġ*', 'ĠTo', 'Ġchange', 'Ġthis', 'Ġlicense', 'Ġheader', ',', 'Ġchoose', 'ĠLicense', 'ĠHeaders', 'Ġin', 'ĠProject', 'ĠProperties', '.Ċ', 'Ġ*'], top_logprobs=[{'/*Ċ': -4.762409210205078, 'from': -1.1374093294143677, '#': -1.8874093294143677, 'class': -2.012409210205078}, {'Ġ*': -0.41076403856277466, 'ĠĠĠ': -2.41076397895813, '*': -2.66076397895813}, {'ĠTo': -0.5941911935806274, 'ĠFile': -3.219191074371338, 'Ġ': -3.281691074371338}, {'Ġchange': -0.004673866089433432, 'Ġview': -8.504673957824707, 'Ġ': -8.629673957824707}, {'Ġthis': -0.003037602873519063, 'Ġ': -7.628037452697754, 'Ġt': -7.940537452697754}, {'Ġlicense': -0.056132953613996506, 'Ġtemplate': -2.9936330318450928, 'Ġlicens': -7.431132793426514}, {'Ġheader': -0.006603917572647333, 'Ġhe': -6.881603717803955, 'header': -7.069103717803955}, {',': -0.0001658063702052459, ',Ċ': -9.562665939331055, 'Ċ': -11.000165939331055}, {'Ġchoose': -0.0011628062929958105, 'Ġcho': -8.1261625289917, 'Ġ': -8.1261625289917}, {'ĠLicense': -0.008314046077430248, 'ĠL': -5.75831413269043, 'ĠLic': -5.88331413269043}, {'ĠHeaders': -0.002553062280640006, 'Headers': -6.8150529861450195, 'Ġ': -7.6900529861450195}, {'Ġin': -0.00031704644788987935, 'Ġi': -8.875316619873047, 'Ġ': -9.187816619873047}, {'ĠProject': -0.010180451907217503, 'ĠProj': -6.010180473327637, 'Ġ': -6.572680473327637}, {'ĠProperties': -0.004279265645891428, 'ĠSettings': -6.879279136657715, 'Ġ': -7.504279136657715}, {'.Ċ': -0.012154560536146164, '.': -4.574654579162598, 'Ċ': -6.887154579162598}, {'Ġ*': -0.0013250865740701556, 'Ġ*Ċ': -7.9388251304626465, 'Ġ': -8.126324653625488}]), text='/*\\n * To change this license header, choose License Headers in Project Properties.\\n *', stop_reason=None, prompt_logprobs=None)])\n",
      "('created', 1753541731)\n",
      "('model', '/home/syu2/Downloads/tbqwen-3B/qwen2.5_coder_3b_base-lora-20250207-124005-iter0150-testloss0.620')\n",
      "('object', 'text_completion')\n",
      "('system_fingerprint', None)\n",
      "('usage', CompletionUsage(completion_tokens=32, prompt_tokens=26, total_tokens=58, completion_tokens_details=None, prompt_tokens_details=None))\n",
      "Check if the special block self-is crosspond.\n",
      "    */\n",
      "    if (side\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "\n",
    "from openai import OpenAI\n",
    "\n",
    "# Modify OpenAI's API key and API base to use vLLM's API server.\n",
    "openai_api_key = \"EMPTY\"\n",
    "openai_api_base = \"http://localhost:8000/v1\"\n",
    "\n",
    "\n",
    "client = OpenAI(\n",
    "    # defaults to os.environ.get(\"OPENAI_API_KEY\")\n",
    "    api_key=openai_api_key,\n",
    "    base_url=openai_api_base,\n",
    ")\n",
    "\n",
    "models = client.models.list()\n",
    "model = models.data[0].id\n",
    "text_qwen = \"<|fim_prefix|>def fib(n):<|fim_suffix|>   else:\\n        return fib(n - 2) + fib(n - 1)<|fim_middle|><|endoftext|>\"\n",
    "# Completion API\n",
    "completion = client.completions.create(\n",
    "    model=model,\n",
    "    prompt=text_qwen,\n",
    "    echo=False,\n",
    "    n=2,\n",
    "    logprobs=3,\n",
    ")\n",
    "\n",
    "print(completion.choices[0].text)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e58ef036-9737-46fe-a09a-83e46ff5e5eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "10\n",
      "20\n",
      "30\n",
      "40\n",
      "50\n",
      "60\n",
      "70\n",
      "80\n",
      "90\n",
      "100\n",
      "110\n",
      "120\n",
      "130\n",
      "140\n",
      "150\n",
      "160\n",
      "170\n",
      "180\n",
      "190\n",
      "200\n",
      "0\n",
      "10\n",
      "20\n",
      "30\n",
      "40\n",
      "50\n",
      "60\n",
      "70\n",
      "80\n",
      "90\n",
      "100\n",
      "110\n",
      "120\n",
      "130\n",
      "140\n",
      "150\n",
      "160\n",
      "170\n",
      "180\n",
      "190\n",
      "200\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import re\n",
    "import argparse\n",
    "\n",
    "from openai import OpenAI\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def extract_between_strings_inclusive_regex(text, start_pattern, end_pattern):\n",
    "    # Construct the regex pattern to capture the delimiters and the content\n",
    "    # re.escape() is used to treat the delimiters as literal strings\n",
    "    # (.*?) captures any characters non-greedily between the delimiters\n",
    "    pattern = re.escape(start_pattern) + r\"(.*?)\" + re.escape(end_pattern)\n",
    "    match = re.search(pattern, text, re.DOTALL)\n",
    "    if match:\n",
    "        # group(0) returns the entire matched string, including the delimiters\n",
    "        return match.group(0)\n",
    "    return None\n",
    "\n",
    "def generate_completion_vllm(text: str):\n",
    "    completion = client.completions.create(\n",
    "        model=model,\n",
    "        prompt=text,\n",
    "        echo=False,\n",
    "        n=2,\n",
    "        logprobs=3,\n",
    "    )\n",
    "    return completion.choices[0].text\n",
    "\n",
    "\n",
    "# Modify OpenAI's API key and API base to use vLLM's API server.\n",
    "openai_api_key = \"EMPTY\"\n",
    "openai_api_base = \"http://localhost:8000/v1\"\n",
    "\n",
    "client = OpenAI(\n",
    "    # defaults to os.environ.get(\"OPENAI_API_KEY\")\n",
    "    api_key=openai_api_key,\n",
    "    base_url=openai_api_base,\n",
    ")\n",
    "\n",
    "models = client.models.list()\n",
    "model = models.data[0].id\n",
    "\n",
    "icount = 0\n",
    "\n",
    "MODE = 'starcoder2'\n",
    "\n",
    "\n",
    "with open(\"prompt_nosuffix_train_20th.jsonl\", \"w\") as outfile:\n",
    "    \n",
    "    with open(\"prompt_nosuffix_train.jsonl\", 'r', encoding='utf-8') as f:\n",
    "\n",
    "#    data = json.load(f)  # Load the whole file as a list\n",
    "    \n",
    "        for line in f:\n",
    "\n",
    "            \n",
    "            entry = json.loads(line)\n",
    "            \n",
    "            if icount>=201: break\n",
    "            if icount%10==0: print(icount)\n",
    "\n",
    "            try:\n",
    "                #print(entry['content'])\n",
    "                #real_prompt = extract_between_strings_inclusive_regex(entry['content'],\"<fim-prefix>\", \"<fim-middle>\")\n",
    "                real_prompt = extract_between_strings_inclusive_regex(entry['content'],\"<fim-prefix>\", \"<fim-middle>\")\n",
    "                #print(real_prompt)\n",
    "                if MODE=='qwen':\n",
    "                    real_prompt = real_prompt.replace(\"<FIM-PREFIX>\", \"<|fim_prefix|>\") \n",
    "                    #real_prompt = re.sub(\"<FIM-PREFIX>\", \"<|fim_prefix|>\", real_prompt, flags=re.IGNORECASE) \n",
    "                    real_prompt = real_prompt.replace(\"<FIM-MIDDLE>\", \"<|fim_middle|>\") \n",
    "                    \n",
    "                answers = generate_completion_vllm(real_prompt)  \n",
    "                \n",
    "                if MODE=='qwen':\n",
    "                    answers = answers.replace(\"<|fim_prefix|>\", \"<FIM-PREFIX>\")\n",
    "                    answers = answers.replace(\"<|fim_suffix|>\", \"<FIM-SUFFIX>\") \n",
    "                    answers = answers.replace(\"<|fim_middle|>\", \"<FIM-MIDDLE>\") \n",
    "                    \n",
    "                entry['completions'] = answers\n",
    "                \n",
    "                # if icount<10:\n",
    "                #     print('==============================')\n",
    "                #     print(real_prompt)\n",
    "                #     print('------------------------------')\n",
    "                #     print(answers)\n",
    "\n",
    "                    \n",
    "                if isinstance(entry, dict):\n",
    "                    outfile.write(json.dumps(entry, separators=(',', ':')))\n",
    "                    outfile.write(\"\\n\")\n",
    "                else:\n",
    "                    print(\"Skipped non-dict entry:\", entry)                \n",
    "\n",
    "                #alljson.append(entry)\n",
    "                icount+=1\n",
    "            except Exception as err:\n",
    "                print(err)\n",
    "                continue\n",
    "\n",
    "\n",
    "\n",
    "icount = 0\n",
    "\n",
    "\n",
    "with open(\"prompt_nosuffix_test_20th.jsonl\", \"w\") as outfile:\n",
    "    \n",
    "    with open(\"prompt_nosuffix_test.jsonl\", 'r', encoding='utf-8') as f:\n",
    "\n",
    "#    data = json.load(f)  # Load the whole file as a list\n",
    "    \n",
    "        for line in f:\n",
    "            \n",
    "            entry = json.loads(line)\n",
    "            \n",
    "            if icount>=201: break\n",
    "            if icount%10==0: print(icount)\n",
    "\n",
    "            try:\n",
    "                #print(entry['content'])\n",
    "                #real_prompt = extract_between_strings_inclusive_regex(entry['content'],\"<fim-prefix>\", \"<fim-middle>\")\n",
    "                real_prompt = extract_between_strings_inclusive_regex(entry['content'],\"<fim-prefix>\", \"<fim-middle>\")\n",
    "                #print(real_prompt)\n",
    "                if MODE=='qwen':\n",
    "                    real_prompt = real_prompt.replace(\"<FIM-PREFIX>\", \"<|fim_prefix|>\") \n",
    "                    #real_prompt = re.sub(\"<FIM-PREFIX>\", \"<|fim_prefix|>\", real_prompt, flags=re.IGNORECASE) \n",
    "                    real_prompt = real_prompt.replace(\"<FIM-MIDDLE>\", \"<|fim_middle|>\") \n",
    "                    \n",
    "                answers = generate_completion_vllm(real_prompt)  \n",
    "                \n",
    "                if MODE=='qwen':\n",
    "                    answers = answers.replace(\"<|fim_prefix|>\", \"<FIM-PREFIX>\")\n",
    "                    answers = answers.replace(\"<|fim_suffix|>\", \"<FIM-SUFFIX>\") \n",
    "                    answers = answers.replace(\"<|fim_middle|>\", \"<FIM-MIDDLE>\") \n",
    "                    \n",
    "                entry['completions'] = answers\n",
    "                \n",
    "                # if icount<10:\n",
    "                #     print('==============================')\n",
    "                #     print(real_prompt)\n",
    "                #     print('------------------------------')\n",
    "                #     print(answers)\n",
    "\n",
    "                    \n",
    "                if isinstance(entry, dict):\n",
    "                    outfile.write(json.dumps(entry, separators=(',', ':')))\n",
    "                    outfile.write(\"\\n\")\n",
    "                else:\n",
    "                    print(\"Skipped non-dict entry:\", entry)                \n",
    "\n",
    "                #alljson.append(entry)\n",
    "                icount+=1\n",
    "            except Exception as err:\n",
    "                print(err)\n",
    "                continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f31481cd-dcb4-4642-b116-164d80e16fc8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d8c859c-1c3e-4603-8232-5e5cd596362d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbf85c8f-02bf-44dc-aeec-494c75d26956",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "refact",
   "language": "python",
   "name": "refact"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
